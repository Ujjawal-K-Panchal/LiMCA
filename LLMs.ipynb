{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9c2870-1e84-4382-b28a-2c6d2321a6d5",
   "metadata": {},
   "source": [
    "# LLM File."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2de5c16-1e8b-4f6b-b85c-b354676ef93d",
   "metadata": {},
   "source": [
    "# Install Requirements if this is your first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e90710-65c1-42ee-a2a5-1f9b021254fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if your first run, uncomment this stuff.\n",
    "# !pip uninstall -y transformers\n",
    "# !pip uninstall -y accelerate\n",
    "# !pip uninstall -y peft\n",
    "# !pip uninstall -y bitsandbytes\n",
    "# !pip uninstall -y torch\n",
    "\n",
    "# !pip install torch#==1.13.0\n",
    "# !pip install transformers \n",
    "# !pip install peft\n",
    "# !pip install bitsandbytes\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad0d190-807c-406b-95f4-0c35365c1101",
   "metadata": {},
   "source": [
    "# Import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598afa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ujjawal/LiMCA/venv4limca/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, transformers, peft, torch, signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c8ad1-f2dd-4519-a09e-f72c59dad0ee",
   "metadata": {},
   "source": [
    "# Some Global Variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb779b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = \"colab\" #\"mac\"\n",
    "#llmname = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "llmname = \"susnato/phi-2\"#codellama/CodeLlama-7b-hf\"\n",
    "#llmname= \"microsoft/phi-2\"\n",
    "#llmname=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = \"cuda:0\" if platform == \"colab\" else \"mps:0\"\n",
    "modelstore = \"./models\"\n",
    "max_seq_len = 4096\n",
    "alpha = 16\n",
    "rank = 8\n",
    "max_new_tokens = 500\n",
    "if not os.path.exists(modelstore):\n",
    "    os.makedirs(modelstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c855d867-b202-499d-806b-661cffbd5324",
   "metadata": {},
   "source": [
    "# Model Loading Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "894d23a1-9fc9-45c3-a211-84d59cd10a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token():\n",
    "     return \"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\"\n",
    "\n",
    "def get_tokenizer(name: str = llmname, model_max_length: int = max_seq_len):\n",
    "\ttok = transformers.AutoTokenizer.from_pretrained(\n",
    "\t\tname,\n",
    "\t\tcache_dir = modelstore,\n",
    "\t\tmodel_max_length = model_max_length,\n",
    "\t\ttoken = get_token()\n",
    "\t)\n",
    "\ttok.padding_side = 'right'\n",
    "\ttok.model_max_length = max_seq_len\n",
    "\treturn tok\n",
    "\n",
    "def get_model(name: str = llmname, quantize: bool | str = \"qlora\"):\n",
    "    if isinstance(quantize, bool):\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            name,\n",
    "            cache_dir = modelstore,\n",
    "            token = get_token()\n",
    "        )\n",
    "    if quantize == True:\n",
    "        model = model.to(torch.float16)\n",
    "    elif quantize == \"qlora\":\n",
    "        nf4_config = transformers.BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype = torch.float16,\n",
    "            )\n",
    "        lora_config = peft.LoraConfig(\n",
    "                r = rank,\n",
    "                lora_alpha = alpha,\n",
    "                target_modules = [\"q_proj\", \"v_proj\"],\n",
    "                bias = \"none\",\n",
    "                task_type = \"CAUSAL_LM\",\n",
    "            )\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            name,\n",
    "            cache_dir = modelstore,\n",
    "            quantization_config = nf4_config,\n",
    "            token = get_token(),\n",
    "        )\n",
    "        model = peft.get_peft_model(model, lora_config)\n",
    "        model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce7997d-07af-46bc-b7c0-0be934c3f514",
   "metadata": {},
   "source": [
    "# Get Yo' Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b198eba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.97s/it]\n"
     ]
    }
   ],
   "source": [
    "#get model and tokenizer.\n",
    "tok = get_tokenizer()\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e776d-456b-49af-9510-846059339ef4",
   "metadata": {},
   "source": [
    "# Some Useful Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a05e6358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_custom_steps(tok, model, genids, input_ids, skip_special_tokens, plen):\n",
    "    if \"codellama\" in model.config.name_or_path:\n",
    "        filling = tok.batch_decode(genids[:, input_ids.shape[1]:], skip_special_tokens = skip_special_tokens)[0]\n",
    "        reply = filling.split(\"\\n\\n\")\n",
    "        if len(temp)>1:\n",
    "            reply=reply[1]\n",
    "        elif len(reply)==1:\n",
    "            reply=reply[0]\n",
    "        else:\n",
    "            reply=reply\n",
    "    else:\n",
    "        reply = tok.decode(genids.squeeze(), skip_special_tokens = skip_special_tokens)#[plen:]\n",
    "    return reply\n",
    "\n",
    "def get_model_output(model, tok, prompt, max_new_tokens: int = max_new_tokens):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Take in LLM and tokenizer and prompt and give me output.\n",
    "    Args:\n",
    "        1. model: llm.\n",
    "        2. tok: tokenizer.\n",
    "        3. min_length: minimum length of output.\n",
    "    \"\"\"\n",
    "    #prompt = f\"\"\"You are a rockstar. Chat with the user bruv. User: '{prompt}'. Pls don't make your reply too long. Reply:\"\"\"\n",
    "    plen = len(prompt)\n",
    "    PROMPT = tok(prompt, return_tensors = \"pt\").to(model.device)\n",
    "    input_ids = PROMPT['input_ids']\n",
    "    reply = None\n",
    "    genids = model.generate(input_ids, max_new_tokens= max_new_tokens)\n",
    "    reply = model_custom_steps(tok, model, genids, input_ids, skip_special_tokens = True,  plen = plen)\n",
    "    return reply\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33d1ecd4-565c-4fd6-80a8-8414ebb12d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Handlers.SIG_DFL: 0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_prompt = \"#'Return a fixed version of the following code. The problem is \"\n",
    "refactory_problems = [\"Sequential Search\", \"Unique Days/months\", \"Remove Duplicates\", \"Sort Tuples\", \"Top-k\"]\n",
    "llm_list = [\"GPT\",\"Phi\", \"Llama\"]\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "def execute_function_definition(definition):\n",
    "    exec(definition, globals())\n",
    "\n",
    "def compare_results(result, expected_result):\n",
    "    return str(result) == str(expected_result).strip()\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError(\"Timed out\")\n",
    "\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8095b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select LLM with Integer Input: 1 for GPT 3.5 | 2 for Code-Phi | 3 for Code-Llama 1\n",
      "Which Refactory Problem is being tested? 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "(('January', '1'), ('January', '2'))\n",
      "(('January', '1'),)\n",
      "(('February', '1'), ('February', '10'))\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "(('January', '1'), ('January', '2'))\n",
      "(('January', '1'),)\n",
      "(('February', '1'), ('February', '10'))\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n"
     ]
    }
   ],
   "source": [
    "llm_number = input(\"Select LLM with Integer Input: 1 for GPT 3.5 | 2 for Code-Phi | 3 for Code-Llama\")\n",
    "problem_number = input(\"Which Refactory Problem is being tested?\")\n",
    "max_iter=1\n",
    "llm_results=dict()\n",
    "\n",
    "prompt=base_prompt + refactory_problems[int(problem_number)-1]\n",
    "directory_name = \"refactory_q\"+problem_number\n",
    "\n",
    "#function_definition_folder = directory_name+\"/wrong_files\"\n",
    "function_definition_folder = directory_name+\"/wrong_files\"\n",
    "function_defs = []\n",
    "for filename in os.listdir(function_definition_folder):\n",
    "    file_path = os.path.join(function_definition_folder, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        function_defs.append(read_file(file_path))\n",
    "\n",
    "\n",
    "# Get function calls from a folder\n",
    "function_call_folder = directory_name+\"/inputs\"\n",
    "function_calls = []\n",
    "for filename in sorted(os.listdir(function_call_folder)):\n",
    "    file_path = os.path.join(function_call_folder, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        function_calls.append(read_file(file_path))\n",
    "\n",
    "# Get expected results from another folder\n",
    "expected_results_folder = directory_name+\"/outputs\"\n",
    "expected_results = []\n",
    "for filename in sorted(os.listdir(expected_results_folder)):\n",
    "    file_path = os.path.join(expected_results_folder, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        expected_results.append(read_file(file_path))\n",
    "\n",
    "# Ensure the number of function calls matches the number of expected results\n",
    "if len(function_calls) != len(expected_results):\n",
    "    print(\"Error: Number of function calls does not match the number of expected results.\")\n",
    "    exit()\n",
    "\n",
    "filenum=0\n",
    "for function_def in function_defs:\n",
    "    filenum+=1\n",
    "    iter=0\n",
    "    function_code=function_def\n",
    "    num_expected_results = 0\n",
    "    llm_results[\"file\"+str(filenum)]=[]\n",
    "    if(os.path.isfile('./'+\"file\"+str(filenum))):\n",
    "        continue\n",
    "    while num_expected_results!= len(expected_results) and iter<max_iter:\n",
    "        skip_tests=0\n",
    "        # Counter for the number of expected results\n",
    "        num_expected_results = 0\n",
    "        model_prompt = prompt + \":\\n\" + function_code# + \"[INST]<FILL_ME>[INST]'\"\n",
    "\n",
    "        # print(f\"{model_prompt=}\")\n",
    "\n",
    "        llm_fixed_code=get_model_output(model, tok, model_prompt)\n",
    "        #BUGFIX: I put this shit inside get_model_output. Because this hack is model-specific. Diff models could have diff gimmicks.\n",
    "        # temp=llm_fixed_code.split(\"\\n\\n\")\n",
    "        # if len(temp)>1:\n",
    "        #     llm_fixed_code=temp[1]\n",
    "        # elif len(temp)==1:\n",
    "        #     llm_fixed_code=temp[0]\n",
    "        # else:\n",
    "        #     llm_fixed_code=temp\n",
    "        # print(f\"{llm_fixed_code=}\")\n",
    "\n",
    "        llm_fixed_code=function_code\n",
    "\n",
    "        try:\n",
    "            # Execute the function definition\n",
    "            execute_function_definition(llm_fixed_code)\n",
    "        except:\n",
    "            num_expected_results=0\n",
    "            skip_tests=1\n",
    "\n",
    "        if not skip_tests:\n",
    "            # Iterate through pairs of function calls and expected results\n",
    "            for function_call, expected_result in zip(function_calls, expected_results):\n",
    "                signal.alarm(10)\n",
    "                # Execute the function call and capture the result\n",
    "                try:\n",
    "                    result = eval(function_call)\n",
    "                    # Compare the result with the expected result\n",
    "                    if compare_results(str(result), expected_result):\n",
    "                        num_expected_results += 1\n",
    "                except:\n",
    "                    pass\n",
    "                finally:\n",
    "                    signal.alarm(0)\n",
    "        #function_code=llm_fixed_code\n",
    "        iter+=1\n",
    "        llm_results[\"file\"+str(filenum)].append(num_expected_results)\n",
    "        if num_expected_results==len(expected_results):\n",
    "            while iter<max_iter:\n",
    "                llm_results[\"file\"+str(filenum)].append(num_expected_results)\n",
    "                iter+=1\n",
    "\n",
    "        #print(\"Number of expected results: for file \", filenum, \": \", num_expected_results,\"/\",len(expected_results))\n",
    "\n",
    "    #print(function_code)\n",
    "    f=open(\"file\"+str(filenum), \"w\")\n",
    "    f.write(function_code)\n",
    "    f.close()\n",
    "\n",
    "    f2=open(\"results\", \"a\")\n",
    "    f2.write(''.join(str(e) for e in llm_results[\"file\"+str(filenum)])+', ')\n",
    "    f2.close()\n",
    "    print(len(llm_results))\n",
    "\n",
    "print(llm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycode_similar\n",
    "\n",
    "\n",
    "function_definition_folder = \"refactory_q5/mistral_solutions\"\n",
    "function_defs = []\n",
    "for filename in os.listdir(function_definition_folder):\n",
    "    file_path = os.path.join(function_definition_folder, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        function_defs.append(read_file(file_path))\n",
    "\n",
    "\n",
    "# Get function calls from a folder\n",
    "function_call_folder = \"refactory_q5/wrong_files\"\n",
    "function_calls = []\n",
    "for filename in sorted(os.listdir(function_call_folder)):\n",
    "    file_path = os.path.join(function_call_folder, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        function_calls.append(read_file(file_path))\n",
    "\n",
    "for function_call, function_def in zip(function_calls, function_defs):\n",
    "    try:\n",
    "        result = pycode_similar.detect([function_call, function_def], diff_method=pycode_similar.UnifiedDiff,keep_prints=True, module_level=True)\n",
    "        print(result[0][1][0].plagiarism_percent)\n",
    "\n",
    "        f2=open(\"plagiarism\", \"a\")\n",
    "        f2.write(str(result[0][1][0].plagiarism_percent)+', ')\n",
    "        f2.close()\n",
    "    except:\n",
    "        f2=open(\"plagiarism\", \"a\")\n",
    "        f2.write(str(.5)+', ')\n",
    "        f2.close()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
