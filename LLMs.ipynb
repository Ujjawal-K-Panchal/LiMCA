{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9c2870-1e84-4382-b28a-2c6d2321a6d5",
   "metadata": {},
   "source": [
    "# LLM File."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2de5c16-1e8b-4f6b-b85c-b354676ef93d",
   "metadata": {},
   "source": [
    "# Install Requirements if this is your first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14e90710-65c1-42ee-a2a5-1f9b021254fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if your first run, uncomment this stuff.\n",
    "# !pip uninstall -y transformers\n",
    "# !pip uninstall -y accelerate\n",
    "# !pip uninstall -y peft\n",
    "# !pip uninstall -y bitsandbytes\n",
    "# !pip uninstall -y torch\n",
    "\n",
    "# !pip install torch#==1.13.0\n",
    "# !pip install transformers \n",
    "# !pip install peft\n",
    "# !pip install bitsandbytes\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad0d190-807c-406b-95f4-0c35365c1101",
   "metadata": {},
   "source": [
    "# Import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "598afa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, transformers, peft, torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c8ad1-f2dd-4519-a09e-f72c59dad0ee",
   "metadata": {},
   "source": [
    "# Some Global Variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbb779b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = \"colab\" #\"mac\"\n",
    "#llmname = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "llmname = \"codellama/CodeLlama-7b-Python-hf\"\n",
    "#llmname= \"microsoft/phi-2\"\n",
    "device = \"cuda:0\" if platform == \"colab\" else \"mps:0\"\n",
    "modelstore = \"./models\"\n",
    "max_seq_len = 4096\n",
    "alpha = 16\n",
    "rank = 8\n",
    "if not os.path.exists(modelstore):\n",
    "    os.makedirs(modelstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e776d-456b-49af-9510-846059339ef4",
   "metadata": {},
   "source": [
    "# Some Useful Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a05e6358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token():\n",
    "     return \"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\"\n",
    "\n",
    "def get_tokenizer(name: str = llmname, model_max_length: int = max_seq_len):\n",
    "\ttok = transformers.AutoTokenizer.from_pretrained(\n",
    "\t\tname,\n",
    "\t\tcache_dir = modelstore,\n",
    "\t\tmodel_max_length = model_max_length,\n",
    "\t\ttoken = get_token()\n",
    "\t)\n",
    "\ttok.padding_side = 'right'\n",
    "\ttok.model_max_length = max_seq_len\n",
    "\treturn tok\n",
    "\n",
    "def get_model(name: str = llmname, quantize: bool | str = \"qlora\"):\n",
    "    if isinstance(quantize, bool):\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            name,\n",
    "            cache_dir = modelstore,\n",
    "            token = get_token()\n",
    "        )\n",
    "    if quantize == True:\n",
    "        model = model.to(torch.float16)\n",
    "    elif quantize == \"qlora\":\n",
    "        nf4_config = transformers.BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype = torch.float16,\n",
    "            )\n",
    "        lora_config = peft.LoraConfig(\n",
    "                r = rank,\n",
    "                lora_alpha = alpha,\n",
    "                target_modules = [\"q_proj\", \"v_proj\"],\n",
    "                bias = \"none\",\n",
    "                task_type = \"CAUSAL_LM\",\n",
    "            )\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            name,\n",
    "            cache_dir = modelstore,\n",
    "            quantization_config = nf4_config,\n",
    "            token = get_token(),\n",
    "        )\n",
    "        model = peft.get_peft_model(model, lora_config)\n",
    "        model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def get_model_output(model, tok, prompt):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Take in LLM and tokenizer and prompt and give me output.\n",
    "    Args:\n",
    "        1. model: llm.\n",
    "        2. tok: tokenizer.\n",
    "        3. min_length: minimum length of output.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a rockstar. Chat with the user bruv. User: '{prompt}'. Pls don't make your reply too long. Reply:\"\"\"\n",
    "    plen = len(prompt)\n",
    "    prompt = tok(prompt, return_tensors = \"pt\").to(model.device)\n",
    "    return tok.decode(model.generate(prompt[\"input_ids\"], do_sample = True).squeeze(), skip_special_tokens = True)[plen:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b198eba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.74s/it]\n"
     ]
    }
   ],
   "source": [
    "#get model and tokenizer.\n",
    "tok = get_tokenizer()\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3d47ee9-2edd-4e10-bda1-3b5a7f12f97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/skotamre/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Input length of input_ids is 39, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " '\n"
     ]
    }
   ],
   "source": [
    "print(get_model_output(model, tok, \"Hi! Wassup mate?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33d1ecd4-565c-4fd6-80a8-8414ebb12d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem \"\n",
    "refactory_problems = [\"Sequential Search\", \"Unique Days/months\", \"Remove Duplicates\", \"Sort Tuples\", \"Top-k\"]\n",
    "llm_list = [\"GPT\",\"Phi\", \"Llama\"]\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "def execute_function_definition(definition):\n",
    "    exec(definition, globals())\n",
    "\n",
    "def compare_results(result, expected_result):\n",
    "    return str(result) == str(expected_result).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b8095b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search: def search(x, seq):\n",
      "    for i, elem in enumerate (seq):\n",
      "        if x<=elem:\n",
      "            return i\n",
      "        elif x>seq[-1]:\n",
      "            return len(seq)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/skotamre/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Input length of input_ids is 89, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search: \n",
      "\n",
      " '\n",
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search:  '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " '\n",
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search: def search(x, seq):\n",
      "    for i, elem in enumerate(seq):\n",
      "        if x <= elem:\n",
      "            return i\n",
      "        elif x > seq[-1]:\n",
      "            return len(seq)\n",
      "\n",
      "\n",
      "\n",
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " '\n",
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search:  '\n",
      " \n",
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search: def search(x, seq):\n",
      "    if seq == ():\n",
      "            return 0\n",
      "    for i in range(len(seq)):\n",
      "        if x > seq[-1]:\n",
      "            return len(seq)\n",
      "        elif x == seq[i]:\n",
      "            return i\n",
      "        elif x < seq[i]:\n",
      "            return i\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skotamre/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Input length of input_ids is 160, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " '\n",
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search:  '\n",
      " ''\n",
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search:  ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/skotamre/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Input length of input_ids is 137, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " '\n",
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search: def search(x, seq):\n",
      "    for a,b in enumerate(seq):\n",
      "        if x<=b:\n",
      "            return a\n",
      "    for i in seq:\n",
      "        if x>i:\n",
      "            return a+1\n",
      "\n",
      " '\"\n",
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search:  '\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I\n",
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search:  I\n",
      " '\n",
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search: def search(x, seq):\n",
      "    if seq == () or []:\n",
      "        indx = 0\n",
      "    else:\n",
      "        if x < seq[0]:\n",
      "            indx = 0\n",
      "        elif x > seq[-1]:\n",
      "            indx = seq.index(seq[-1]) + 1\n",
      "        else:\n",
      "            for i in seq:\n",
      "                if x <= i:\n",
      "                    indx = (seq.index(i))\n",
      "                    break                    \n",
      "        return indx\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skotamre/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Input length of input_ids is 198, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"\n",
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search:  \"\n",
      " '\n",
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search:  '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/skotamre/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Input length of input_ids is 136, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " '\n",
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search: def search(x, seq):\n",
      "    for i,elem in enumerate(seq):\n",
      "        if elem<x:\n",
      "            pos=i+1\n",
      "        elif elem>x:\n",
      "            pos=i\n",
      "    return pos\n",
      "\n",
      " '\n",
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search:  '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " '.\n",
      "You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem Sequential Search:  '.\n",
      " '\n",
      "{'file1': [0, 0, 0], 'file2': [0, 0, 0], 'file3': [0, 0, 0], 'file4': [0, 0, 0], 'file5': [0, 0, 0], 'file6': [0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "llm_number = input(\"Select LLM with Integer Input: 1 for GPT 3.5 | 2 for Code-Phi | 3 for Code-Llama\")\n",
    "problem_number = input(\"Which Refactory Problem is being tested?\")\n",
    "max_iter=3\n",
    "llm_results=dict()\n",
    "\n",
    "\n",
    "prompt=base_prompt + refactory_problems[int(problem_number)-1]\n",
    "directory_name = \"refactory_q\"+problem_number\n",
    "\n",
    "function_definition_folder = directory_name+\"/wrong_files\"\n",
    "#function_definition_folder = directory_name+\"/GPT_sol_iter_1\"\n",
    "function_defs = []\n",
    "for filename in os.listdir(function_definition_folder):\n",
    "    file_path = os.path.join(function_definition_folder, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        function_defs.append(read_file(file_path))\n",
    "\n",
    "\n",
    "# Get function calls from a folder\n",
    "function_call_folder = directory_name+\"/inputs\"\n",
    "function_calls = []\n",
    "for filename in sorted(os.listdir(function_call_folder)):\n",
    "    file_path = os.path.join(function_call_folder, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        function_calls.append(read_file(file_path))\n",
    "\n",
    "# Get expected results from another folder\n",
    "expected_results_folder = directory_name+\"/outputs\"\n",
    "expected_results = []\n",
    "for filename in sorted(os.listdir(expected_results_folder)):\n",
    "    file_path = os.path.join(expected_results_folder, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        expected_results.append(read_file(file_path))\n",
    "\n",
    "# Ensure the number of function calls matches the number of expected results\n",
    "if len(function_calls) != len(expected_results):\n",
    "    print(\"Error: Number of function calls does not match the number of expected results.\")\n",
    "    exit()\n",
    "\n",
    "filenum=0\n",
    "for function_def in function_defs:\n",
    "    filenum+=1\n",
    "    iter=0\n",
    "    function_code=function_def\n",
    "    num_expected_results = 0\n",
    "    llm_results[\"file\"+str(filenum)]=[]\n",
    "    while num_expected_results!= len(expected_results) and iter<max_iter:\n",
    "        skip_tests=0\n",
    "        # Counter for the number of expected results\n",
    "        num_expected_results = 0\n",
    "        model_prompt = prompt + \": \" + function_code\n",
    "\n",
    "        #print(model_prompt)\n",
    "\n",
    "        llm_fixed_code=get_model_output(model, tok, model_prompt)\n",
    "        print(llm_fixed_code)\n",
    "\n",
    "\n",
    "        try:\n",
    "            # Execute the function definition\n",
    "            execute_function_definition(llm_fixed_code)\n",
    "        except:\n",
    "            num_expected_results=0\n",
    "            skip_tests=1\n",
    "\n",
    "        if not skip_tests:\n",
    "            # Iterate through pairs of function calls and expected results\n",
    "            for function_call, expected_result in zip(function_calls, expected_results):\n",
    "                # Execute the function call and capture the result\n",
    "                try:\n",
    "                    result = eval(function_call)\n",
    "                    # Compare the result with the expected result\n",
    "                    if compare_results(str(result), expected_result):\n",
    "                        num_expected_results += 1\n",
    "                except:\n",
    "                    pass\n",
    "        function_code=llm_fixed_code\n",
    "        iter+=1\n",
    "        llm_results[\"file\"+str(filenum)].append(num_expected_results)\n",
    "        if num_expected_results==len(expected_results):\n",
    "            while iter<max_iter:\n",
    "                llm_results[\"file\"+str(filenum)].append(num_expected_results)\n",
    "                iter+=1\n",
    "\n",
    "        #print(\"Number of expected results: for file \", filenum, \": \", num_expected_results,\"/\",len(expected_results))\n",
    "\n",
    "    if filenum>5:\n",
    "        break\n",
    "print(llm_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
