{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9c2870-1e84-4382-b28a-2c6d2321a6d5",
   "metadata": {},
   "source": [
    "# LLM File."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2de5c16-1e8b-4f6b-b85c-b354676ef93d",
   "metadata": {},
   "source": [
    "# Install Requirements if this is your first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e90710-65c1-42ee-a2a5-1f9b021254fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if your first run, uncomment this stuff.\n",
    "# !pip uninstall -y transformers\n",
    "# !pip uninstall -y accelerate\n",
    "# !pip uninstall -y peft\n",
    "# !pip uninstall -y bitsandbytes\n",
    "# !pip uninstall -y torch\n",
    "\n",
    "# !pip install torch#==1.13.0\n",
    "# !pip install transformers \n",
    "# !pip install peft\n",
    "# !pip install bitsandbytes\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad0d190-807c-406b-95f4-0c35365c1101",
   "metadata": {},
   "source": [
    "# Import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598afa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ujjawal/LiMCA/venv4limca/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, transformers, peft, torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c8ad1-f2dd-4519-a09e-f72c59dad0ee",
   "metadata": {},
   "source": [
    "# Some Global Variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb779b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = \"colab\" #\"mac\"\n",
    "#llmname = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "llmname = \"codellama/CodeLlama-7b-hf\"\n",
    "#llmname= \"microsoft/phi-2\"\n",
    "device = \"cuda:0\" if platform == \"colab\" else \"mps:0\"\n",
    "modelstore = \"./models\"\n",
    "max_seq_len = 4096\n",
    "alpha = 16\n",
    "rank = 8\n",
    "max_new_tokens = 500\n",
    "if not os.path.exists(modelstore):\n",
    "    os.makedirs(modelstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e776d-456b-49af-9510-846059339ef4",
   "metadata": {},
   "source": [
    "# Some Useful Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a05e6358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token():\n",
    "     return \"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\"\n",
    "\n",
    "def get_tokenizer(name: str = llmname, model_max_length: int = max_seq_len):\n",
    "\ttok = transformers.AutoTokenizer.from_pretrained(\n",
    "\t\tname,\n",
    "\t\tcache_dir = modelstore,\n",
    "\t\tmodel_max_length = model_max_length,\n",
    "\t\ttoken = get_token()\n",
    "\t)\n",
    "\ttok.padding_side = 'right'\n",
    "\ttok.model_max_length = max_seq_len\n",
    "\treturn tok\n",
    "\n",
    "def get_model(name: str = llmname, quantize: bool | str = \"qlora\"):\n",
    "    if isinstance(quantize, bool):\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            name,\n",
    "            cache_dir = modelstore,\n",
    "            token = get_token()\n",
    "        )\n",
    "    if quantize == True:\n",
    "        model = model.to(torch.float16)\n",
    "    elif quantize == \"qlora\":\n",
    "        nf4_config = transformers.BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype = torch.float16,\n",
    "            )\n",
    "        lora_config = peft.LoraConfig(\n",
    "                r = rank,\n",
    "                lora_alpha = alpha,\n",
    "                target_modules = [\"q_proj\", \"v_proj\"],\n",
    "                bias = \"none\",\n",
    "                task_type = \"CAUSAL_LM\",\n",
    "            )\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            name,\n",
    "            cache_dir = modelstore,\n",
    "            quantization_config = nf4_config,\n",
    "            token = get_token(),\n",
    "        )\n",
    "        model = peft.get_peft_model(model, lora_config)\n",
    "        model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def get_model_output(model, tok, prompt, max_new_tokens: int = max_new_tokens):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Take in LLM and tokenizer and prompt and give me output.\n",
    "    Args:\n",
    "        1. model: llm.\n",
    "        2. tok: tokenizer.\n",
    "        3. min_length: minimum length of output.\n",
    "    \"\"\"\n",
    "    #prompt = f\"\"\"You are a rockstar. Chat with the user bruv. User: '{prompt}'. Pls don't make your reply too long. Reply:\"\"\"\n",
    "    plen = len(prompt)\n",
    "    PROMPT = tok(prompt, return_tensors = \"pt\").to(model.device)\n",
    "    input_ids = PROMPT['input_ids']\n",
    "    reply = None\n",
    "    if \"codellama\" in model.config.name_or_path:\n",
    "        generated_ids = model.generate(input_ids, max_new_tokens= max_new_tokens)\n",
    "        filling = tok.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]\n",
    "        reply = prompt.replace(\"<FILL_ME>\", filling)\n",
    "        reply = reply.split(\"[INST]\")[1]\n",
    "    else:\n",
    "        reply = tok.decode(model.generate(input_ids, do_sample = True, max_length = max_new_tokens + plen).squeeze(), skip_special_tokens = True)[plen:]\n",
    "    return reply\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b198eba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.92s/it]\n"
     ]
    }
   ],
   "source": [
    "#get model and tokenizer.\n",
    "tok = get_tokenizer()\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3d47ee9-2edd-4e10-bda1-3b5a7f12f97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def print_hello_world():\n",
      "    print(\"Hello World!\")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#For CodeLlama.\n",
    "print(get_model_output(model, tok, \"\"\"[INST]\n",
    "def print_hello_world():<FILL_ME>\n",
    "[/INST]\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d1ecd4-565c-4fd6-80a8-8414ebb12d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"You must only return code with no other supporting text, do not offer any interpretations or responses other than the fixed code. You must maintain as much of the original code as possible and cannot perform large modifications. Fix the bugs in the following problem \"\n",
    "refactory_problems = [\"Sequential Search\", \"Unique Days/months\", \"Remove Duplicates\", \"Sort Tuples\", \"Top-k\"]\n",
    "llm_list = [\"GPT\",\"Phi\", \"Llama\"]\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "def execute_function_definition(definition):\n",
    "    exec(definition, globals())\n",
    "\n",
    "def compare_results(result, expected_result):\n",
    "    return str(result) == str(expected_result).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8095b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_number = input(\"Select LLM with Integer Input: 1 for GPT 3.5 | 2 for Code-Phi | 3 for Code-Llama\")\n",
    "problem_number = input(\"Which Refactory Problem is being tested?\")\n",
    "max_iter=3\n",
    "llm_results=dict()\n",
    "\n",
    "\n",
    "prompt=base_prompt + refactory_problems[int(problem_number)-1]\n",
    "directory_name = \"refactory_q\"+problem_number\n",
    "\n",
    "function_definition_folder = directory_name+\"/wrong_files\"\n",
    "#function_definition_folder = directory_name+\"/GPT_sol_iter_1\"\n",
    "function_defs = []\n",
    "for filename in os.listdir(function_definition_folder):\n",
    "    file_path = os.path.join(function_definition_folder, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        function_defs.append(read_file(file_path))\n",
    "\n",
    "\n",
    "# Get function calls from a folder\n",
    "function_call_folder = directory_name+\"/inputs\"\n",
    "function_calls = []\n",
    "for filename in sorted(os.listdir(function_call_folder)):\n",
    "    file_path = os.path.join(function_call_folder, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        function_calls.append(read_file(file_path))\n",
    "\n",
    "# Get expected results from another folder\n",
    "expected_results_folder = directory_name+\"/outputs\"\n",
    "expected_results = []\n",
    "for filename in sorted(os.listdir(expected_results_folder)):\n",
    "    file_path = os.path.join(expected_results_folder, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        expected_results.append(read_file(file_path))\n",
    "\n",
    "# Ensure the number of function calls matches the number of expected results\n",
    "if len(function_calls) != len(expected_results):\n",
    "    print(\"Error: Number of function calls does not match the number of expected results.\")\n",
    "    exit()\n",
    "\n",
    "filenum=0\n",
    "for function_def in function_defs:\n",
    "    filenum+=1\n",
    "    iter=0\n",
    "    function_code=function_def\n",
    "    num_expected_results = 0\n",
    "    llm_results[\"file\"+str(filenum)]=[]\n",
    "    while num_expected_results!= len(expected_results) and iter<max_iter:\n",
    "        skip_tests=0\n",
    "        # Counter for the number of expected results\n",
    "        num_expected_results = 0\n",
    "        model_prompt = prompt + \": \" + function_code\n",
    "\n",
    "        #print(model_prompt)\n",
    "\n",
    "        llm_fixed_code=get_model_output(model, tok, model_prompt)\n",
    "        print(llm_fixed_code)\n",
    "\n",
    "\n",
    "        try:\n",
    "            # Execute the function definition\n",
    "            execute_function_definition(llm_fixed_code)\n",
    "        except:\n",
    "            num_expected_results=0\n",
    "            skip_tests=1\n",
    "\n",
    "        if not skip_tests:\n",
    "            # Iterate through pairs of function calls and expected results\n",
    "            for function_call, expected_result in zip(function_calls, expected_results):\n",
    "                # Execute the function call and capture the result\n",
    "                try:\n",
    "                    result = eval(function_call)\n",
    "                    # Compare the result with the expected result\n",
    "                    if compare_results(str(result), expected_result):\n",
    "                        num_expected_results += 1\n",
    "                except:\n",
    "                    pass\n",
    "        function_code=llm_fixed_code\n",
    "        iter+=1\n",
    "        llm_results[\"file\"+str(filenum)].append(num_expected_results)\n",
    "        if num_expected_results==len(expected_results):\n",
    "            while iter<max_iter:\n",
    "                llm_results[\"file\"+str(filenum)].append(num_expected_results)\n",
    "                iter+=1\n",
    "\n",
    "        #print(\"Number of expected results: for file \", filenum, \": \", num_expected_results,\"/\",len(expected_results))\n",
    "\n",
    "    if filenum>5:\n",
    "        break\n",
    "print(llm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ec859-e814-4952-9a5e-e182b5445926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
